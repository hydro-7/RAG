{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e3c98176",
      "metadata": {
        "id": "e3c98176"
      },
      "source": [
        "# **Local Retrieval Augmented Generation (RAG) from Scratch**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05df1e27",
      "metadata": {
        "id": "05df1e27"
      },
      "source": [
        "## **Importing Libraries**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "BMA9aQ5wYFm_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMA9aQ5wYFm_",
        "outputId": "7f9ec132-ae90-4188-d53c-76fcd34e31ad"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Invalid requirement: '#': Expected package name at the start of dependency specifier\n",
            "    #\n",
            "    ^\n"
          ]
        }
      ],
      "source": [
        "# (Run in Colab to simulate a venv, libraries like Spacy cause a problem due to version dependencies)\n",
        "!pip install -r \"/content/Requirements.txt\" -qq                  # -qq = quiet mode in pip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "675a97f8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 494
        },
        "id": "675a97f8",
        "outputId": "dc7c78b8-b6b6-4ebc-bf95-d71dc7ef1eee"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import fitz\n",
        "from tqdm.auto import tqdm\n",
        "import re\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "import textwrap"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "711aa888",
      "metadata": {
        "id": "711aa888"
      },
      "source": [
        "## **Reading the PDF**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "O2NunnLIc6Ua",
      "metadata": {
        "id": "O2NunnLIc6Ua"
      },
      "source": [
        "To read the PDFs that we import, libraries like **PyMuPDF** and **PyPDF** can be used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f5797a29",
      "metadata": {
        "id": "f5797a29"
      },
      "outputs": [],
      "source": [
        "pdf_path = 'human-nutrition-text.pdf'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af2de3ae",
      "metadata": {
        "id": "af2de3ae"
      },
      "source": [
        "The text formatter **formats the text** read from the PDF by removing '\\n' symbols. Additional changes can be made when required.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4284a8a1",
      "metadata": {
        "id": "4284a8a1"
      },
      "outputs": [],
      "source": [
        "def Text_Format(text : str) -> str:\n",
        "    cleaned_text = text.replace('\\n', '').strip()                   # Strip removes any leading or trailing whitespaces\n",
        "    return cleaned_text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "caae0ac2",
      "metadata": {
        "id": "caae0ac2"
      },
      "source": [
        "The Read_PDF function reads the input PDF and stores it as a list of dictionaries.\\\n",
        "\\\n",
        "Alternative libraries like **NLTK** and **Spacy** can be used to sentencize the given text. These are far more accurate than just using split by fullstops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "9dc0587c",
      "metadata": {
        "id": "9dc0587c"
      },
      "outputs": [],
      "source": [
        "def Read_PDF(pdf_path : str) -> list[dict]:\n",
        "    pdf = fitz.open(pdf_path)\n",
        "    pages_list = []                                                 # The list that stores the page dicts\n",
        "\n",
        "    for page_number, page in (enumerate(pdf)):\n",
        "        text = page.get_text()\n",
        "        text = Text_Format(text)\n",
        "\n",
        "        pages_list.append({\n",
        "            \"page_number\" : page_number - 41,                       # In our PDF, the page numbers start off from page 42, i.e, pg 42 -> pg 1\n",
        "            \"page_char_count\" : len(text),\n",
        "            \"page_word_count\" : len(text.split(\" \")),\n",
        "            \"page_sentence_count\" : len(text.split(\". \")),\n",
        "            \"page_token_count\" : len(text) / 4,                     # According to the GPT OpenAI paper, on average, 1 token ~ 4 char\n",
        "            \"text\" : text,\n",
        "            \"sentences\" : text.split(\". \")\n",
        "        })\n",
        "\n",
        "    return pages_list\n",
        "\n",
        "\n",
        "page_list = Read_PDF(pdf_path = pdf_path);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e8b483a",
      "metadata": {
        "id": "1e8b483a"
      },
      "source": [
        "## **Text Splitting (Chunking)**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bauyXz71c9Lm",
      "metadata": {
        "id": "bauyXz71c9Lm"
      },
      "source": [
        "Here, we group sentences together into groups of 10. This can be done using the LangChain Library if required. \\\n",
        "\\\n",
        "We do this because :\n",
        "- Easier to manage similar sized chunks of text.\n",
        "- Don't overload the embedding models capacity for tokens (e.g. if an embedding model has a capacity of 384 tokens, there could be information loss if you try to embed a sequence of 400+ tokens).\n",
        "- Our LLM context window (the amount of tokens an LLM can take in) may be limited and requires compute power so we want to make sure we're using it as well as possible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "8IOHQugGcMlf",
      "metadata": {
        "id": "8IOHQugGcMlf"
      },
      "outputs": [],
      "source": [
        "chunk_size = 10                                                       # Each chunk has 10 sentences, so any page with > 10 sentences gets split : [17] -> [[10], [7]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "753c9f09",
      "metadata": {
        "id": "753c9f09"
      },
      "outputs": [],
      "source": [
        "def split_list(input_list: list, size: int) -> list[list[str]]:\n",
        "\n",
        "    return [input_list[i:i + size] for i in range(0, len(input_list), size)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "oibPAJWXbzU3",
      "metadata": {
        "id": "oibPAJWXbzU3"
      },
      "outputs": [],
      "source": [
        "for item in page_list:\n",
        "    item[\"sentence_chunks\"] = split_list(input_list = item[\"sentences\"], size = chunk_size)\n",
        "    item[\"num_chunks\"] = len(item[\"sentence_chunks\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Na37AgvUdFRh",
      "metadata": {
        "id": "Na37AgvUdFRh"
      },
      "source": [
        "## **Seperating Chunks to be Individual Pages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "WAVh34eifLF0",
      "metadata": {
        "collapsed": true,
        "id": "WAVh34eifLF0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['should considering their length, and slightly more than 20 percent of children ages two to five are overweight or have obesity.4 Some minority group children, such as Filipinos, Native Hawaiians, and Other Pacific Islanders, in Hawai‘i have higher rates of overweight and obesity',\n",
              "  'In 2012, 12.8% of Hawai‘i WIC (low-income) participants ages two to four years were overweight and 10.2% had obesity.567 One study that investigated 2000-2010 data for children ages two to eight years in 51 communities in 11 United States Affiliated Pacific (USAP) jurisdictions found that 14.4% of the study population was overweight and 14% had obesity.8 4',\n",
              "  'Institute of Medicine',\n",
              "  '(2011)',\n",
              "  'Early childhood obesity prevention policies',\n",
              "  'The National Academies Press',\n",
              "  '5',\n",
              "  'Oshiro C., Novotny R., Grove J., Hurwitz E',\n",
              "  '(2015)',\n",
              "  'Race/ethnic differences in birth size, infant growth, and body mass index at age five years in children in Hawaii'],\n",
              " ['Childhood Obesity, 11(6),683-690',\n",
              "  'https://www.ncbi.nlm.nih.gov/pubmed/26561722 6',\n",
              "  'Thorn B., Tadler C., Huret N., Ayo E., Trippe C',\n",
              "  '(2015, November)',\n",
              "  'WIC participant and program characteristics final report',\n",
              "  'https://fns-prod.azureedge.net/sites/default/files/ops/WICPC2014.pdf 7',\n",
              "  'State of childhood obesity',\n",
              "  'Obesity rates & trend data',\n",
              "  'https://stateofchildhoodobesity.org/data/ 8',\n",
              "  'Novotny R., Fenfang L., Fialkowski, M'],\n",
              " ['(2016)',\n",
              "  'Prevalence of obesity and acanthosis nigricans among young children in the Children’s Healthy Living Program in the United States Affiliated Pacific',\n",
              "  'Medicine, 37, e4711',\n",
              "  '  http://chl-pacific.org/wp-content/uploads/2011/08/Novotny-et-Toddler Years  |  859']]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "page_list[900][\"sentence_chunks\"]\n",
        "# So there are 3 chunks of text, each chunk has several sentences which are stored as list[str]\n",
        "# So sentence_chunks = list[list[str]], where 1 individual chunk = list[str]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "23adVVyudKq_",
      "metadata": {
        "id": "23adVVyudKq_"
      },
      "outputs": [],
      "source": [
        "seperated_chunk_list = []\n",
        "\n",
        "for dic in page_list:\n",
        "    for chunk in dic[\"sentence_chunks\"]:\n",
        "\n",
        "        new_dict = {}\n",
        "        new_dict[\"page_number\"] = dic[\"page_number\"]                        # As page number remains the same\n",
        "\n",
        "        # Making a Paragraph out of the list of strings in 1 chunk\n",
        "        joined_sentence = \"\".join(chunk)                                    # Join together all the sentences in this 1 chunk (Because chunk = list[str])\n",
        "        joined_sentence = joined_sentence.replace(\"  \", \" \").strip()\n",
        "\n",
        "        joined_sentence = re.sub(r'\\.([A-Z])', r'. \\1', joined_sentence)    # After joining, the Sentences have no spacing between them : S1.S2 , so we introduce spaces between .Capital : .A -> . A\n",
        "\n",
        "        new_dict[\"paragraph\"] = joined_sentence                             # new_dict[\"paragraph\"] --> str\n",
        "\n",
        "        # Other keys are similar to the original page dictionary\n",
        "        new_dict[\"char_count\"] = len(joined_sentence)\n",
        "        new_dict[\"word_count\"] = len(joined_sentence.split(\" \"))\n",
        "        new_dict[\"sentence_count\"] = len(joined_sentence.split(\". \"))\n",
        "        new_dict[\"token_count\"] = len(joined_sentence) / 4\n",
        "\n",
        "        seperated_chunk_list.append(new_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "R8GIULpLjukf",
      "metadata": {
        "id": "R8GIULpLjukf"
      },
      "source": [
        "## **Removing Unecessary Chunks**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Pc5dliP1jyeu",
      "metadata": {
        "id": "Pc5dliP1jyeu"
      },
      "source": [
        "The chunks with a **low number of tokens** rarely have valuable information, so we can **remove those chunks** and save ourselves some processing power."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "TjFhwY0rkgxC",
      "metadata": {
        "id": "TjFhwY0rkgxC"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(seperated_chunk_list)\n",
        "min_tokens = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "BX0W8uChjx2g",
      "metadata": {
        "id": "BX0W8uChjx2g"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>page_number</th>\n",
              "      <th>char_count</th>\n",
              "      <th>word_count</th>\n",
              "      <th>sentence_count</th>\n",
              "      <th>token_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1671.00</td>\n",
              "      <td>1671.00</td>\n",
              "      <td>1671.00</td>\n",
              "      <td>1671.00</td>\n",
              "      <td>1671.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>587.46</td>\n",
              "      <td>787.86</td>\n",
              "      <td>116.37</td>\n",
              "      <td>1.05</td>\n",
              "      <td>196.96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>349.87</td>\n",
              "      <td>417.85</td>\n",
              "      <td>65.96</td>\n",
              "      <td>0.25</td>\n",
              "      <td>104.46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-39.00</td>\n",
              "      <td>121.00</td>\n",
              "      <td>3.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>30.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>283.50</td>\n",
              "      <td>400.50</td>\n",
              "      <td>57.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>100.12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>594.00</td>\n",
              "      <td>801.00</td>\n",
              "      <td>118.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>200.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>894.00</td>\n",
              "      <td>1130.00</td>\n",
              "      <td>170.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>282.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1166.00</td>\n",
              "      <td>1863.00</td>\n",
              "      <td>299.00</td>\n",
              "      <td>4.00</td>\n",
              "      <td>465.75</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       page_number  char_count  word_count  sentence_count  token_count\n",
              "count      1671.00     1671.00     1671.00         1671.00      1671.00\n",
              "mean        587.46      787.86      116.37            1.05       196.96\n",
              "std         349.87      417.85       65.96            0.25       104.46\n",
              "min         -39.00      121.00        3.00            1.00        30.25\n",
              "25%         283.50      400.50       57.50            1.00       100.12\n",
              "50%         594.00      801.00      118.00            1.00       200.25\n",
              "75%         894.00     1130.00      170.00            1.00       282.50\n",
              "max        1166.00     1863.00      299.00            4.00       465.75"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pruned_seperated_chunk_list = df[df[\"token_count\"] > min_tokens].to_dict(orient=\"records\")\n",
        "df_pruned = pd.DataFrame(pruned_seperated_chunk_list)\n",
        "df_pruned.describe().round(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "phYDqMy4ljwV",
      "metadata": {
        "id": "phYDqMy4ljwV"
      },
      "source": [
        "## **Adding Chunk Embeddings**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "F2QVGk31orPC",
      "metadata": {
        "id": "F2QVGk31orPC"
      },
      "source": [
        "To add the embeddings, we make use of the **all-mpnet-base-v2 model** from the sentence transformers library. \\\n",
        "\\\n",
        "This library leads to the generation of embeddings of **shape (768, ) with 384 tokens taken in at once**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "OtLdaAFnqCll",
      "metadata": {
        "id": "OtLdaAFnqCll"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8d48a03ec13343f5afce2236d3a4fc06",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\_AI_Stuff\\RAG\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\paart\\.cache\\huggingface\\hub\\models--sentence-transformers--all-mpnet-base-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "84d984ade9a94bed9f33d4fd19e3339b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5a999631ae6b4a9281b9ce9489d794b8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/10.4k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "770282b769354270b5f601a60259b647",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\_AI_Stuff\\RAG\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1bf088ba65d94f1daee8a9e036610a39",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\_AI_Stuff\\RAG\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bfac5e98ecbd40a6b8cbe6245d30f9c0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "091d2e42e70948459c0c9f25c0e231da",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "37d4f4e15091494eb0d9242ab69aed60",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "48f46cee0e434de396bd8ceda93ae33a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a22b8d53e83e4bb8a1d7a93cc8c75ece",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a2da563830cf48018cfc49379cdd1cb4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\", device=\"cpu\");        # Change to GPU to make it faster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "ShWNt5AorhrJ",
      "metadata": {
        "id": "ShWNt5AorhrJ"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a47e5501a0594933bb19703c2a605daa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1671 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: total: 57min 4s\n",
            "Wall time: 11min\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "embedding_model.to(\"cpu\")\n",
        "\n",
        "for item in tqdm(pruned_seperated_chunk_list):\n",
        "    item[\"embedding\"] = embedding_model.encode(item[\"paragraph\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oGDq8XlPrsTD",
      "metadata": {
        "id": "oGDq8XlPrsTD"
      },
      "source": [
        "Alternatively, we can try Batch Operations, this helps reduce the time further while using a gpu."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "jwdZpeo2q8hv",
      "metadata": {
        "id": "jwdZpeo2q8hv"
      },
      "outputs": [],
      "source": [
        "# embedding_model.to(\"cpu\")\n",
        "\n",
        "# text_chunks = [item[\"paragraph\"] for item in pruned_seperated_chunk_list]\n",
        "# text_chunk_embeddings = embedding_model.encode(text_chunks, batch_size=32, convert_to_tensor=True)\n",
        "# text_chunk_embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae2c7555",
      "metadata": {},
      "source": [
        "## **Saving the Embeddings**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9536a842",
      "metadata": {},
      "source": [
        "We save the Dictionary of Chunks along with the Embedding of each chunk, so that we dont need to keep re-calculating the embeddings on every instance. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "51d4cb0a",
      "metadata": {},
      "outputs": [],
      "source": [
        "text_chunks_and_embeddings_df = pd.DataFrame(pruned_seperated_chunk_list)\n",
        "embeddings_df_save_path = \"chunk_embeddings.csv\"\n",
        "text_chunks_and_embeddings_df.to_csv(embeddings_df_save_path, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15b3cdc8",
      "metadata": {},
      "source": [
        "#### **==== NOTE ====**\n",
        "If the CSV file is available, we can start running the notebook from this point. Just re reun the embedding model definition. \n",
        "#### **==============**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10c58352",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>page_number</th>\n",
              "      <th>paragraph</th>\n",
              "      <th>char_count</th>\n",
              "      <th>word_count</th>\n",
              "      <th>sentence_count</th>\n",
              "      <th>token_count</th>\n",
              "      <th>embedding</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-39</td>\n",
              "      <td>Human Nutrition: 2020 Edition UNIVERSITY OF HA...</td>\n",
              "      <td>308</td>\n",
              "      <td>42</td>\n",
              "      <td>1</td>\n",
              "      <td>77.00</td>\n",
              "      <td>[ 6.74242899e-02  9.02280360e-02 -5.09548606e-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-38</td>\n",
              "      <td>Human Nutrition: 2020 Edition by University of...</td>\n",
              "      <td>210</td>\n",
              "      <td>30</td>\n",
              "      <td>1</td>\n",
              "      <td>52.50</td>\n",
              "      <td>[ 5.52156381e-02  5.92138283e-02 -1.66167859e-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-37</td>\n",
              "      <td>Contents Preface University of Hawai‘i at Māno...</td>\n",
              "      <td>762</td>\n",
              "      <td>114</td>\n",
              "      <td>1</td>\n",
              "      <td>190.50</td>\n",
              "      <td>[ 2.68206690e-02  3.37356739e-02 -2.30485611e-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-36</td>\n",
              "      <td>Lifestyles and Nutrition University of Hawai‘i...</td>\n",
              "      <td>937</td>\n",
              "      <td>142</td>\n",
              "      <td>1</td>\n",
              "      <td>234.25</td>\n",
              "      <td>[ 6.77905306e-02  4.26554494e-02 -7.37832859e-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-35</td>\n",
              "      <td>The Cardiovascular System University of Hawai‘...</td>\n",
              "      <td>998</td>\n",
              "      <td>152</td>\n",
              "      <td>1</td>\n",
              "      <td>249.50</td>\n",
              "      <td>[ 3.30264382e-02 -8.49774200e-03  9.57152806e-...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   page_number                                          paragraph  char_count  \\\n",
              "0          -39  Human Nutrition: 2020 Edition UNIVERSITY OF HA...         308   \n",
              "1          -38  Human Nutrition: 2020 Edition by University of...         210   \n",
              "2          -37  Contents Preface University of Hawai‘i at Māno...         762   \n",
              "3          -36  Lifestyles and Nutrition University of Hawai‘i...         937   \n",
              "4          -35  The Cardiovascular System University of Hawai‘...         998   \n",
              "\n",
              "   word_count  sentence_count  token_count  \\\n",
              "0          42               1        77.00   \n",
              "1          30               1        52.50   \n",
              "2         114               1       190.50   \n",
              "3         142               1       234.25   \n",
              "4         152               1       249.50   \n",
              "\n",
              "                                           embedding  \n",
              "0  [ 6.74242899e-02  9.02280360e-02 -5.09548606e-...  \n",
              "1  [ 5.52156381e-02  5.92138283e-02 -1.66167859e-...  \n",
              "2  [ 2.68206690e-02  3.37356739e-02 -2.30485611e-...  \n",
              "3  [ 6.77905306e-02  4.26554494e-02 -7.37832859e-...  \n",
              "4  [ 3.30264382e-02 -8.49774200e-03  9.57152806e-...  "
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "emb = pd.read_csv(\"chunk_embeddings.csv\")\n",
        "emb.head()\n",
        "\n",
        "emb[\"embedding\"] = emb[\"embedding\"].apply(lambda x: np.fromstring(x.strip(\"[]\"), sep=\" \"))      # Convert embedding column back to np.array (it got converted to string when it got saved to CSV)\n",
        "embeddings = torch.tensor(np.array(emb[\"embedding\"].tolist()), dtype=torch.float32).to(\"cpu\")   # (Note: NumPy arrays are float64, torch tensors are float32 by default)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a25fb9f0",
      "metadata": {},
      "source": [
        "## **Retrieval**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b762cb9",
      "metadata": {},
      "source": [
        "Once we have our embeddings, we make a system to **retrieve relevant paragraphs based on quries**.\\\n",
        "\\\n",
        "But before that, even the queries need to be made into embeddings so that they can be compared against the embeddings that we already have. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "29b6008b",
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \"macronutrients functions\"\n",
        "query_embedding = embedding_model.encode(query, convert_to_tensor=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d395e11c",
      "metadata": {},
      "source": [
        "Getting the **Cosine Similarity** and storing the **top k** results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "3465184d",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.return_types.topk(\n",
              "values=tensor([0.6785, 0.6673, 0.6564, 0.6522, 0.6444]),\n",
              "indices=tensor([42, 52, 41, 46, 51]))"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dot_scores = util.dot_score(a = query_embedding, b = embeddings)[0]\n",
        "top_results_dot_product = torch.topk(dot_scores, k = 5)\n",
        "top_results_dot_product"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "7d055865",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score: 0.6785\n",
            "Text:\n",
            "Macronutrients Nutrients that are needed in large amounts are called\n",
            "macronutrientsThere are three classes of macronutrients: carbohydrates, lipids,\n",
            "and proteinsThese can be metabolically processed into cellular energyThe energy\n",
            "from macronutrients comes from their chemical bondsThis chemical energy is\n",
            "converted into cellular energy that is then utilized to perform work, allowing\n",
            "our bodies to conduct their basic functionsA unit of measurement of food energy\n",
            "is the calorieOn nutrition food labels the amount given for “calories” is\n",
            "actually equivalent to each calorie multiplied by one thousandA kilocalorie (one\n",
            "thousand calories, denoted with a small “c”) is synonymous with the “Calorie”\n",
            "(with a capital “C”) on nutrition food labelsWater is also a macronutrient in\n",
            "the sense that you require a large amount of it, but unlike the other\n",
            "macronutrients, it does not yield caloriesCarbohydrates Carbohydrates are\n",
            "molecules composed of carbon, hydrogen, and oxygen\n",
            "Page number: 5\n",
            "\n",
            "\n",
            "Score: 0.6673\n",
            "Text:\n",
            "Protein Necessary for tissue formation, cell reparation, and hormone and enzyme\n",
            "productionIt is essential for building strong muscles and a healthy immune\n",
            "systemCarbohydrates Provide a ready source of energy for the body and provide\n",
            "structural constituents for the formation of cellsFat Provides stored energy for\n",
            "the body, functions as structural components of cells and also as signaling\n",
            "molecules for proper cellular communicationIt provides insulation to vital\n",
            "organs and works to maintain body temperatureVitamins Regulate body processes\n",
            "and promote normal body-system functionsMinerals Regulate body processes, are\n",
            "necessary for proper cellular function, and comprise body tissueWater Transports\n",
            "essential nutrients to all body parts, transports waste products for disposal,\n",
            "and aids with body temperature maintenance Learning Activities Technology Note:\n",
            "The second edition of the Human Nutrition Open Educational Resource (OER)\n",
            "textbook features interactive learning activities These activities are available\n",
            "in the web-based textbook and not available in the downloadable versions (EPUB,\n",
            "Digital PDF, Print_PDF, or Open Document)\n",
            "Page number: 12\n",
            "\n",
            "\n",
            "Score: 0.6564\n",
            "Text:\n",
            "Learning Objectives By the end of this chapter, you will be able to: • Describe\n",
            "basic concepts in nutrition • Describe factors that affect your nutritional\n",
            "needs • Describe the importance of research and scientific methods to\n",
            "understanding nutrition What are Nutrients? The foods we eat contain\n",
            "nutrientsNutrients are substances required by the body to perform its basic\n",
            "functionsNutrients must be obtained from our diet, since the human body does not\n",
            "synthesize or produce themNutrients have one or more of three basic functions:\n",
            "they provide energy, contribute to body structure, and/or regulate chemical\n",
            "processes in the bodyThese basic functions allow us to detect and respond to\n",
            "environmental surroundings, move, excrete wastes, respire (breathe), grow, and\n",
            "reproduceThere are six classes of nutrients required for the body to function\n",
            "and maintain overall healthThese are carbohydrates, lipids, proteins, water,\n",
            "vitamins, and mineralsFoods also contain non-nutrients that may be harmful (such\n",
            "as natural toxins common in plant foods and additives like some dyes and\n",
            "preservatives) or beneficial (such as antioxidants)4 | Introduction\n",
            "Page number: 4\n",
            "\n",
            "\n",
            "Score: 0.6522\n",
            "Text:\n",
            "Figure 1.1 The Macronutrients: Carbohydrates, Lipids, Protein, and Water\n",
            "Proteins Proteins are macromolecules composed of chains of subunits called amino\n",
            "acidsAmino acids are simple subunits composed of carbon, oxygen, hydrogen, and\n",
            "nitrogenFood sources of proteins include meats, dairy products, seafood, and a\n",
            "variety of different plant-based foods, most notably soyThe word protein comes\n",
            "from a Greek word meaning “of primary importance,” which is an apt description\n",
            "of these macronutrients; they are also known colloquially as the “workhorses” of\n",
            "lifeProteins provide four kilocalories of energy per gram; however providing\n",
            "energy is not protein’s most important functionProteins provide structure to\n",
            "bones, muscles and skin, and play a role in conducting most of the chemical\n",
            "reactions that take place in the bodyScientists estimate that greater than one-\n",
            "hundred thousand different proteins exist within the human bodyThe genetic codes\n",
            "in DNA are basically protein recipes that determine the order in which 20\n",
            "different amino acids are bound together to make thousands of specific\n",
            "proteinsFigure 1.1 The Macronutrients: Carbohydrates, Lipids, Protein, and Water\n",
            "Introduction | 7\n",
            "Page number: 7\n",
            "\n",
            "\n",
            "Score: 0.6444\n",
            "Text:\n",
            "Vitamins Major Functions Water-soluble Thiamin (B1) Coenzyme, energy metabolism\n",
            "assistance Riboflavin (B2 ) Coenzyme, energy metabolism assistance Niacin (B3)\n",
            "Coenzyme, energy metabolism assistance Pantothenic acid (B5) Coenzyme, energy\n",
            "metabolism assistance Pyridoxine (B6) Coenzyme, amino acid synthesis assistance\n",
            "Biotin (B7) Coenzyme, amino acid and fatty acid metabolism Folate (B9) Coenzyme,\n",
            "essential for growth Cobalamin (B12) Coenzyme, red blood cell synthesis C\n",
            "(ascorbic acid) Collagen synthesis, antioxidant Fat-soluble A Vision,\n",
            "reproduction, immune system function D Bone and teeth health maintenance, immune\n",
            "system function E Antioxidant, cell membrane protection K Bone and teeth health\n",
            "maintenance, blood clotting Vitamin deficiencies can cause severe health\n",
            "problems and even deathFor example, a deficiency in niacin causes a disease\n",
            "called pellagra, which was common in the early twentieth century in some parts\n",
            "of AmericaThe common signs and symptoms of pellagra are known as the\n",
            "“4D’s—diarrhea, dermatitis, dementia, and death.” Until scientists found out\n",
            "that better diets relieved the signs and symptoms of pellagra, many people with\n",
            "the disease ended up hospitalized in insane asylums awaiting deathOther vitamins\n",
            "were also found to prevent certain disorders and diseases such as scurvy\n",
            "(vitamin C), night blindness vitamin A, and rickets (vitamin D)Table 1.3\n",
            "Functions of Nutrients Introduction | 11\n",
            "Page number: 11\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def print_wrapped(text, wrap_length=80):\n",
        "    wrapped_text = textwrap.fill(text, wrap_length)\n",
        "    print(wrapped_text)\n",
        "\n",
        "for score, idx in zip(top_results_dot_product[0], top_results_dot_product[1]):\n",
        "    print(f\"Score: {score:.4f}\")\n",
        "    # Print relevant sentence chunk (since the scores are in descending order, the most relevant chunk will be first)\n",
        "    print(\"Text:\")\n",
        "    print_wrapped(pruned_seperated_chunk_list[idx][\"paragraph\"])\n",
        "    # Print the page number too so we can reference the textbook further (and check the results)\n",
        "    print(f\"Page number: {pruned_seperated_chunk_list[idx]['page_number']}\")\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d08dc3b",
      "metadata": {},
      "source": [
        "Apart from this top-k ranking, we can also construct a re-ranking model that looks at the top-k and re-ranks them, further improving the accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5b7804f",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
